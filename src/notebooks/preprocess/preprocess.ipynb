{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(f'pd=={pd.__version__}')\n",
    "print(f'np=={np.__version__}')\n",
    "print(f'sns=={sns.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in train and test datasets\n",
    "train_transaction = pd.read_csv('/Users/oskarwallberg/desktop/kaggle-datasets/ieee-fraud-detection/train_transaction.csv', index_col='TransactionID')\n",
    "test_transaction = pd.read_csv('/Users/oskarwallberg/desktop/kaggle-datasets/ieee-fraud-detection/test_transaction.csv', index_col='TransactionID')\n",
    "\n",
    "test_transaction.columns = train_transaction.columns.drop(labels='isFraud') # ensure congruent columns\n",
    "\n",
    "train_transaction.shape, test_transaction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_transaction = train_transaction.loc[train_transaction['isFraud']==1]\n",
    "valid_transaction = train_transaction.loc[train_transaction['isFraud']==0]\n",
    "assert fraud_transaction.shape[0] + valid_transaction.shape[0] == train_transaction.shape[0]\n",
    "fraud_transaction.shape, valid_transaction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAUD_RATE = fraud_transaction.shape[0] / train_transaction.shape[0]\n",
    "FRAUD_RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing steps\n",
    "#   1. Encoding features as numerical\n",
    "#   2. Filling NaNs\n",
    "#   3. NOTE SKIP! FE, transforming, enriching, creating features\n",
    "#   4. Selecting features (FS)\n",
    "#   5. SMOTE and other over-/undersampling techniques\n",
    "#   6. Outlier detection and filtering\n",
    "# Done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Encoding Possibilities\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical encoding\n",
    "object_columns = train_transaction.select_dtypes(include=object).columns\n",
    "binary_object_columns = ['M1', 'M2', 'M3', 'M5', 'M6', 'M7', 'M8', 'M9'] # M1-M3, M5-M9, encode as 0/1 for False/True\n",
    "diverse_object_columns = ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'M4'] # EDA and encode as best suited\n",
    "\n",
    "for col in object_columns:\n",
    "    print(col, train_transaction[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diverse object columns EDA\n",
    "def make_pivot(df:pd.DataFrame, index:str) -> pd.DataFrame:\n",
    "    \"\"\"...\"\"\"\n",
    "    df[index] = df[index].fillna('NaN')\n",
    "    df_pivot = df.pivot_table(index=index, columns='isFraud', aggfunc='size')\n",
    "    df_pivot = df_pivot.fillna(0)\n",
    "    df_pivot.columns = ['valid', 'fraud']\n",
    "    df_pivot['count'] = df_pivot['valid'] + df_pivot['fraud']\n",
    "    df_pivot[['valid_norm', 'fraud_norm']] = df_pivot[['valid', 'fraud']].div(other=df_pivot['count'], axis=0)\n",
    "    df_pivot = df_pivot.sort_values(by='fraud_norm', ascending=False)\n",
    "    return df_pivot\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding ProductCD (product code feature)\n",
    "pivot_prodcd = make_pivot(df=train_transaction, index='ProductCD')\n",
    "pivot_prodcd\n",
    "\n",
    "# NOTE Highest rate of fraud in descending order: C -> S -> H -> R -> W\n",
    "# C : 11.7% : 68519\n",
    "# S :  5.9% : 11628\n",
    "# H :  4.8% : 33024\n",
    "# R :  3.8% : 37699\n",
    "# W :  2.0% : 439670\n",
    "\n",
    "# Could encode as hierarchical with (C, S, H, R, W, NaN) -> (5, 4, 3, 2, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding card4 (payment card type)\n",
    "pivot_card4 = make_pivot(df=train_transaction, index='card4')\n",
    "pivot_card4\n",
    "\n",
    "# NOTE Highest rate of fraud in descending order: discover -> visa -> mastercard -> american express\n",
    "# discover\t        0.077282\n",
    "# visa\t            0.034756\n",
    "# mastercard        0.034331\n",
    "# american express  0.028698\n",
    "\n",
    "# Could encode as (discover, visa, mastercard, express, NaN) -> (4, 3, 2, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding card6 ()\n",
    "pivot_card6 = make_pivot(df=train_transaction, index='card6')\n",
    "pivot_card6\n",
    "\n",
    "# NOTE Create two columns : credit, debit - two flag columns \n",
    "# all 'debit or credit' will become 'debit'\n",
    "# charge card is encoded as 0, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding P_emaildomain ()\n",
    "pivot_pdomain = make_pivot(df=train_transaction, index='P_emaildomain')\n",
    "pivot_pdomain = pd.concat([pivot_pdomain, pd.DataFrame(data=[FRAUD_RATE], index=['scranton.edu'], columns=['fraud_norm'])])\n",
    "pivot_pdomain.head(60)\n",
    "# Ideas for how to encode:\n",
    "#   1. Split domain strings into domain name, ending, landcode and potentially other types as separate features\n",
    "#       - if ends with com / net \n",
    "#       - if ends witt 2 letter word (country code)\n",
    "#       - first item is domain host\n",
    "#   2. Order by fraud rate and partition into groups after some threshold values\n",
    "#   3. Map each full domain string to its fraud rate - happen to match test data quite well\n",
    "\n",
    "# Sketch of splitting domain strings...\n",
    "# train_transaction.loc[train_transaction['P_emaildomain'].str.contains('.'), 'P_emaildomain'].str.split('.')\n",
    "# df['domain_type'] = df['email'].str.extract(r'(.com|.org)$')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train P                    train R                   test P                     test R                   \n",
    "# aim.com                315 aim.com                36 aim.com                153 aim.com                41\n",
    "# anonymous.com        36998 anonymous.com       20529 anonymous.com        34064 anonymous.com       19115\n",
    "# aol.com              28289 aol.com              3701 aol.com              24048 aol.com              3538\n",
    "# att.net               4033 att.net               430 att.net               3614 att.net               440\n",
    "# bellsouth.net         1909 bellsouth.net         422 bellsouth.net         1528 bellsouth.net         373\n",
    "# cableone.net           159 cableone.net           27 cableone.net           152 cableone.net           19\n",
    "# centurylink.net        205 centurylink.net        12 centurylink.net        181 centurylink.net        16\n",
    "# cfl.rr.com             172 cfl.rr.com             37 cfl.rr.com             146 cfl.rr.com             20\n",
    "# charter.net            816 charter.net           127 charter.net            627 charter.net           136\n",
    "# comcast.net           7888 comcast.net          1812 comcast.net           6586 comcast.net          1701\n",
    "# cox.net               1393 cox.net               459 cox.net               1264 cox.net               395\n",
    "# earthlink.net          514 earthlink.net          79 earthlink.net          465 earthlink.net          91\n",
    "# embarqmail.com         260 embarqmail.com         68 embarqmail.com         204 embarqmail.com         72\n",
    "# frontier.com           280 frontier.com           52 frontier.com           314 frontier.com           58\n",
    "# frontiernet.net        195 frontiernet.net        14 frontiernet.net        202 frontiernet.net        24\n",
    "# gmail                  496 gmail                  95 gmail                  497 gmail                 101\n",
    "# gmail.com           228355 gmail.com           57147 gmail.com           207448 gmail.com           61738\n",
    "# gmx.de                 149 gmx.de                147 gmx.de                 149 gmx.de                150\n",
    "# hotmail.co.uk          112 hotmail.co.uk         105 hotmail.co.uk          222 hotmail.co.uk         212\n",
    "# hotmail.com          45250 hotmail.com         27509 hotmail.com          40399 hotmail.com         25657\n",
    "# hotmail.de              43 hotmail.de             42 hotmail.de              87 hotmail.de             88\n",
    "# hotmail.es             305 hotmail.es            292 hotmail.es             322 hotmail.es            303\n",
    "# hotmail.fr             295 hotmail.fr            293 hotmail.fr             379 hotmail.fr            374\n",
    "# icloud.com            6267 icloud.com           1398 icloud.com            6049 icloud.com           1422\n",
    "# juno.com               322 juno.com               53 juno.com               252 juno.com               58\n",
    "# live.com              3041 live.com              762 live.com              2679 live.com              682\n",
    "# live.com.mx            749 live.com.mx           754 live.com.mx            721 live.com.mx           710\n",
    "# live.fr                 56 live.fr                55 live.fr                 50 live.fr                50\n",
    "# mac.com                436 mac.com               218 mac.com                426 mac.com               212\n",
    "# mail.com               559 mail.com              122 mail.com               597 mail.com              219\n",
    "# me.com                1522 me.com                556 me.com                1191 me.com                539\n",
    "# msn.com               4092 msn.com               852 msn.com               3388 msn.com               846\n",
    "# netzero.com            230 netzero.com            14 netzero.com            157 netzero.com            10\n",
    "# netzero.net            196 netzero.net             9 netzero.net            123 netzero.net            10\n",
    "# optonline.net         1011 optonline.net         187 optonline.net          926 optonline.net         163\n",
    "# outlook.com           5096 outlook.com          2507 outlook.com           4838 outlook.com          2504\n",
    "# outlook.es             438 outlook.es            433 outlook.es             425 outlook.es            420\n",
    "# prodigy.net.mx         207 prodigy.net.mx        207 prodigy.net.mx          96 prodigy.net.mx         96\n",
    "# protonmail.com          76 protonmail.com         41 protonmail.com          83 protonmail.com         34\n",
    "# ptd.net                 68 ptd.net                27 ptd.net                 72 ptd.net                43\n",
    "# q.com                  189 q.com                  25 q.com                  173 q.com                  20\n",
    "# roadrunner.com         305 roadrunner.com         53 roadrunner.com         278 roadrunner.com         48\n",
    "# rocketmail.com         664 rocketmail.com         69 rocketmail.com         441 rocketmail.com         57\n",
    "# sbcglobal.net         2970 sbcglobal.net         552 sbcglobal.net         2797 sbcglobal.net         611\n",
    "# sc.rr.com              164 sc.rr.com               8 sc.rr.com              113 sc.rr.com               6\n",
    "#   * NOTE MISSING *         scranton.edu           63 scranton.edu             2 scranton.edu            6\n",
    "# servicios-ta.com        35 servicios-ta.com       35 servicios-ta.com        45 servicios-ta.com       45\n",
    "# suddenlink.net         175 suddenlink.net         25 suddenlink.net         148 suddenlink.net         30\n",
    "# twc.com                230 twc.com                29 twc.com                209 twc.com                32\n",
    "# verizon.net           2705 verizon.net           620 verizon.net           2306 verizon.net           582\n",
    "# web.de                 240 web.de                237 web.de                 278 web.de                277\n",
    "# windstream.net         305 windstream.net         47 windstream.net         247 windstream.net         57\n",
    "# yahoo.co.jp             32 yahoo.co.jp            33 yahoo.co.jp             69 yahoo.co.jp            71\n",
    "# yahoo.co.uk             49 yahoo.co.uk            39 yahoo.co.uk             54 yahoo.co.uk            43\n",
    "# yahoo.com           100934 yahoo.com           11842 yahoo.com            81850 yahoo.com            9563\n",
    "# yahoo.com.mx          1543 yahoo.com.mx         1508 yahoo.com.mx          1284 yahoo.com.mx         1235\n",
    "# yahoo.de                74 yahoo.de               75 yahoo.de                63 yahoo.de               64\n",
    "# yahoo.es               134 yahoo.es               57 yahoo.es               138 yahoo.es               67\n",
    "# yahoo.fr               143 yahoo.fr              137 yahoo.fr               201 yahoo.fr              178\n",
    "# ymail.com             2396 ymail.com             207 ymail.com             1679 ymail.com             198\n",
    "\n",
    "# domain name -> fraud rate\n",
    "# impute mean fraud rate for scranton...\n",
    "# if unknown -> 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode R_emaildomain (receiver email domain)\n",
    "pivot_rdomain = make_pivot(df=train_transaction, index='R_emaildomain')\n",
    "pivot_rdomain\n",
    "\n",
    "# Same method here as for P_emaildomain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transaction['R_emaildomain'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode M4 (match 4)\n",
    "pivot_m4 = make_pivot(df=train_transaction, index='M4')\n",
    "pivot_m4\n",
    "\n",
    "# Hierarchical encoding (M2, M0, M1, NaN) -> (...rates...)\n",
    "# Map each value to its fraud rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the Features to Numerical Values\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction_pp = train_transaction.copy()\n",
    "train_transaction_pp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode binary features T/F/NaN as 1/0/-1\n",
    "train_transaction_pp[binary_object_columns] = train_transaction[binary_object_columns].map(lambda x: 1 if x=='T' else 0 if x=='F' else -1)\n",
    "train_transaction_pp[binary_object_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map from index to series values - from product codes to fraud rates\n",
    "train_transaction_pp['ProductCD'] = train_transaction['ProductCD'].map(pivot_prodcd['fraud_norm'])\n",
    "train_transaction_pp['card4'] = train_transaction['card4'].map(pivot_card4['fraud_norm'])\n",
    "train_transaction_pp['card6'] = train_transaction['card6'].map(pivot_card6['fraud_norm'])\n",
    "train_transaction_pp['M4'] = train_transaction['M4'].map(pivot_m4['fraud_norm'])\n",
    "\n",
    "# encode emaildomain features\n",
    "\n",
    "# p_domain_lists = train_transaction['P_emaildomain'].fillna('NaN').str.split('.')\n",
    "\n",
    "# NOTE EXCLUDE NOTE  p_country_code = p_domain_lists.transform(lambda l: l[-1] if len(l[-1])==2 else np.NaN) # NOTE Majority is NaN! Only 4% has country codes...\n",
    "# p_email_host = p_domain_lists.transform(lambda l: l[0] if l[0]!='NaN' else np.NaN)\n",
    "# p_email_tld = p_domain_lists.transform(lambda l: 'com' if 'com' in l else 'net' if 'net' in l else np.NaN) # TLD stands for 'top level domain'\n",
    "# train_transaction_pp['P_email_host'] = p_email_host\n",
    "# train_transaction_pp['P_email_tld'] = p_email_tld\n",
    "\n",
    "# map domain names to fraud rate - impute for the one missing\n",
    "train_transaction_pp['P_domain_fraud_rate'] = train_transaction['P_emaildomain'].map(pivot_pdomain['fraud_norm'])\n",
    "\n",
    "# partition fraud rates into degree of risks\n",
    "# <1%           low risk 0\n",
    "# >=1% & <5% medium risk 1\n",
    "# >=5%         high risk 2\n",
    "# NaNs       medium risk 1\n",
    "train_transaction_pp['P_domain_risk_group'] = train_transaction_pp['P_domain_fraud_rate'].transform(lambda x: 0 if x<0.01 else 1 if 0.01<=x<0.05 else 2 if 0.05<=x else np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diverse_encoded_columns = ['ProductCD', 'card4', 'card6', 'P_domain_fraud_rate', 'P_domain_risk_group', 'M4']\n",
    "train_transaction_pp[diverse_encoded_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction_pp = train_transaction_pp.drop(columns=train_transaction_pp.select_dtypes(object).columns) # faster this way since fewer object column, 2 to 390-ish numerical\n",
    "train_transaction_pp.select_dtypes(object).columns # should be no object columns left!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing steps\n",
    "#   1. Encoding features as numerical\n",
    "#   2. Filling NaNs\n",
    "#   3. NOTE SKIP! FE, transforming, enriching, creating features\n",
    "#   4. Selecting features\n",
    "#   5. SMOTE and other over-/undersampling techniques\n",
    "#   6. Outlier detection and filtering\n",
    "# Done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling NaNs\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This many columns contains some form of NaNs\n",
    "train_transaction_pp.isna().any(axis=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple solution to impute all columns not containing -1 with -1 for NaNs\n",
    "# NOTE lazy solution but a quick fix and can be iterated on further down the line\n",
    "is_nan = train_transaction_pp.isna().any(axis=0)\n",
    "is_neg = train_transaction_pp.lt(0).any(axis=0)\n",
    "columns_nan_special = train_transaction_pp.loc[:, (is_nan & is_neg)].columns # the columns both containing NaNs to be patched and that already contains negative values, in need of a different solution\n",
    "columns_nan_normal = train_transaction_pp.loc[:, (is_nan & ~is_neg)].columns\n",
    "print(f'Normal imputing (-1): {columns_nan_normal.shape[0]}')\n",
    "print(f'Special imputing (x): {columns_nan_special.shape[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction_pp[columns_nan_normal] = train_transaction_pp[columns_nan_normal].fillna(-1)\n",
    "print(f'NaN cols remaining: {train_transaction_pp.isna().any(axis=0).sum()}') # remaining NaN columns requiring special handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remarkably few negative values in each of the columns...\n",
    "# XXX Are negative Timedeltas reasonable in this scenario or improper data?\n",
    "train_transaction_pp[columns_nan_special].lt(0).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All 25th percentiles are at 0, is probably the most reasonable imputation besides the value -1\n",
    "train_transaction_pp[columns_nan_special].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction_pp[list(columns_nan_special)+['isFraud']].loc[train_transaction_pp[columns_nan_special].lt(0).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering the still substantial number of values, the few negative may as well be treated as outliers. Well wait with this claim for now and see later...\n",
    "train_transaction_pp[columns_nan_special].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data=train_transaction_pp[columns_nan_special].isna(), cbar=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 12))\n",
    "fig.suptitle(\"NaN special case columns - Distribution of Values\", fontweight=\"bold\", fontsize=16)\n",
    "gs = plt.GridSpec(nrows=3, ncols=2)\n",
    "for i, ax_loc in enumerate(gs):\n",
    "    col = columns_nan_special[i]\n",
    "    ax = fig.add_subplot(ax_loc)\n",
    "    ax.set_title(col)\n",
    "    train_transaction_pp[col].hist(bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction_pp[columns_nan_special] = train_transaction_pp[columns_nan_special].fillna(-1) # -1 still seems to be the best impute value still (maybe 0 / the mode)\n",
    "print(f'NaN cols remaining: {train_transaction_pp.isna().any(axis=0).sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems OK!\n",
    "train_transaction_pp.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering, SKIP For now!\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be implemented...\n",
    "# NOTE for future_\n",
    "#   : add DXn columns as shown by winning competitors - perhaps those versions are more interpretable for the model??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matricies\n",
    "columns_vesta = np.array([f\"V{i}\" for i in range(1, 340)])\n",
    "columns_non_vesta = train_transaction_pp.columns.drop(labels=columns_vesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr_heatmap(df:pd.DataFrame, annot=False, figsize=(12, 10)):\n",
    "    \"\"\"Create and plot heatmap of feature correlation matrix\"\"\"\n",
    "    corr = df.corr()\n",
    "    \n",
    "    mask = np.zeros_like(corr, dtype=bool) # not masking any at the moment, all masking is false\n",
    "    triu_index = np.triu_indices_from(mask) # argument to mask upper triangle of correlation matrix, redundant otherwise\n",
    "    mask[triu_index] = True\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    sns.heatmap(data=corr, mask=mask, vmin=-1, center=0, vmax=1, cmap=\"coolwarm\", cbar=True, annot=annot, fmt=\".2g\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr_heatmap(df=train_transaction_pp[columns_non_vesta])\n",
    "# From this investivation, the suspect column families are:\n",
    "#   : count columns (CX)\n",
    "#   : match columns (MX)\n",
    "# We view them separately, the reason being that viewing all column as once becomes unreadable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_cx_drop = [\"C1\", \"C2\", \"C6\", \"C7\", \"C8\", \"C9\", \"C10\", \"C11\", \"C14\"] # candidates to drop, no correlation over 90%\n",
    "columns_cx = [f\"C{i}\" for i in range(1, 15)]\n",
    "columns_cx_keep = list(set(columns_cx) - set(columns_cx_drop))\n",
    "plot_corr_heatmap(df=train_transaction_pp[[\"isFraud\"]+columns_cx_keep], annot=True, figsize=(6, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_mx_drop = [\"M2\", \"M3\", \"M7\", \"M9\"] # cadidates to drop, no correlation over 90%\n",
    "columns_mx = [f\"M{i}\" for i in range(1, 10)]\n",
    "columns_mx_keep = list(set(columns_mx) - set(columns_mx_drop))\n",
    "plot_corr_heatmap(df=train_transaction_pp[[\"isFraud\"]+columns_mx_keep], annot=True, figsize=(6, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D1 and D2 strongly correlated, drop D1 since weaker correlation with isFraud\n",
    "train_transaction_pp[[\"isFraud\", \"D1\", \"D2\"]].corr()\n",
    "columns_dx_drop = [\"D1\"]\n",
    "columns_dx = [f\"D{i}\" for i in range(1, 16)]\n",
    "columns_dx_keep = list(set(columns_dx)-set(columns_dx_drop))\n",
    "plot_corr_heatmap(df=train_transaction_pp[[\"isFraud\"]+columns_dx_keep], annot=True, figsize=(12, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_corr_heatmap(df=train_transaction_pp[[\"isFraud\"]+list(columns_v100)], annot=True, figsize=(18, 16))\n",
    "corr_vesta = train_transaction_pp[['isFraud']+list(columns_vesta)].corr()\n",
    "corr_vesta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# might keep, many corr: \n",
    "# 'V173', 'V188'!, 'V191', 'V194', 'V195', 'V197', 'V198'\n",
    "# 'V223', 'V240', 'V241', 'V242'!, 'V244'!, 'V247', 'V248', 'V249', 'V250', 'V251', 'V252'\n",
    "\n",
    "# might keep, some corr:\n",
    "# ... only above 0.2\n",
    "# 'V170', 'V171', 'V176', 'V188', 'V189', 'V190', 'V199', 'V200', 'V201',\n",
    "# 'V228', 'V230', 'V242', 'V243', 'V244', 'V246', 'V257', 'V258', \n",
    "\n",
    "# might keep, few corr:\n",
    "# 'V44', 'V45', 'V111', 'V123', 'V125', 'V169', 'V170', 'V171', 'V175', 'V181', 'V185', 'V187',\n",
    "# 'V220', 'V222', 'V229', 'V235', 'V238', 'V239', 'V258'!!, 'V282', 'V283', 'V303', 'V304',\n",
    "\n",
    "\n",
    "# columns_corr_drop_axis1 = corr_vesta.gt(0.9).sum(axis=1) + corr_vesta.lt(-0.9).sum(axis=1) \n",
    "num_high_corr_by_column = corr_vesta.gt(0.9).sum(axis=0) + corr_vesta.lt(-0.9).sum(axis=0) - 1 # minus 1 since all columns have 100% correlation with itself\n",
    "\n",
    "# Quick filter for features to drop\n",
    "# if less than 0.07 correlation with isFraud and more than 3 highly correlated features -> Arrata!\n",
    "feature_corr_threshold = num_high_corr_by_column.gt(3)  # multicollinearity\n",
    "isfraud_corr_threshold = corr_vesta['isFraud'].lt(0.07) # to little impact\n",
    "\n",
    "vesta_drop_mask = feature_corr_threshold & isfraud_corr_threshold\n",
    "\n",
    "columns_vx_drop = num_high_corr_by_column.loc[vesta_drop_mask].index.to_list()\n",
    "columns_vx_keep = num_high_corr_by_column.loc[~vesta_drop_mask].index.to_list()\n",
    "\n",
    "len(columns_vx_drop), len(columns_vx_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 Vesta features correlated with isFraud\n",
    "plot_corr_heatmap(df=train_transaction_pp[corr_vesta['isFraud'].sort_values(ascending=False)[:20].index], annot=True, figsize=(16, 14))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of columns to drop from the correlation analysis. \n",
    "columns_corr_drop = columns_cx_drop+columns_mx_drop+columns_dx_drop+columns_vx_drop\n",
    "len(columns_corr_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name explanation: preprocessed (pp) train transaction data after 1st round of feature selection (fs1)\n",
    "train_transaction_pp_fs1 = train_transaction_pp.drop(columns=columns_corr_drop).copy()\n",
    "train_transaction_pp_fs1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset is now 1.2 GB instead of 1.7 GB!\n",
    "train_transaction_pp_fs1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using LR w. Lasso (L1) regularisation for Feature Selection\n",
    "* LR is practical for binary labels (isFraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(\n",
    "    penalty=\"l1\",\n",
    "    solver=\"saga\",\n",
    "    C=1,\n",
    "    max_iter=100,\n",
    "    verbose=True,\n",
    "    n_jobs=-1,\n",
    "    class_weight={0: 1, 1:4},\n",
    "    # Cs=2,           # inverse regularisation strengths, logarithmic range of 10 values\n",
    "    # cv=3,           # CV folds\n",
    "    # penalty=\"l1\",   # Lasso (L1) regularisation\n",
    "    # solver=\"saga\",  # better for large dataset, liblinear for small datasets\n",
    "    # max_iter=100,   # adjust for strong regularisation (especially for saga solver)\n",
    "    # verbose=True,\n",
    "    # n_jobs=-1,\n",
    ")\n",
    "\n",
    "logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_transaction_pp_fs1['isFraud'].copy()\n",
    "X = train_transaction_pp_fs1.drop(columns='isFraud').copy()\n",
    "\n",
    "X_sc = pd.DataFrame(\n",
    "    index=X.index,\n",
    "    columns=X.columns,\n",
    "    data=sc.fit_transform(X=X),\n",
    ")\n",
    "\n",
    "X_sc.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE RELEVANT FOR LATER ON\n",
    "# logreg.fit(X_sc, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE RELEVANT FOR LATER ON\n",
    "# feature_importance = pd.Series(data=logreg.coef_[0], index=X_sc.columns).abs().sort_values(ascending=False)\n",
    "# for (index, importance) in feature_importance.items():\n",
    "#     print(index, importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction_pp_fs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_dataset_path = \"/Users/oskarwallberg/Desktop/kaggle-datasets/ieee-fraud-detection/\"\n",
    "os.listdir(path=kaggle_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_data_pp_fs1 = \"train_transaction_pp_fs1.csv\"\n",
    "train_transaction_pp_fs1.to_csv(path_or_buf=kaggle_dataset_path+name_data_pp_fs1, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
